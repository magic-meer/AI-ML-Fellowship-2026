{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10: Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data preprocessing and feature engineering are crucial steps in preparing data for ML models. This notebook covers common challenges and techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset with missing values\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    'Age': [25, 30, np.nan, 45, 35, np.nan, 50, 28, 33, 40],\n",
    "    'Salary': [50000, 60000, 55000, np.nan, 70000, 45000, np.nan, 52000, 58000, 65000],\n",
    "    'Department': ['IT', 'HR', 'IT', 'Sales', np.nan, 'HR', 'IT', 'Sales', 'HR', 'IT'],\n",
    "    'Experience': [2, 5, 3, 10, 7, 1, 15, 4, 6, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(df)\n",
    "print(f\"\\nMissing values per column:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Mean/Median Imputation?\n",
    "\n",
    "Mean imputation is suitable when data is **MCAR (Missing Completely at Random)** and has no extreme outliers. Median is preferred when data has **outliers** as it's more robust to skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# For numerical: use median (robust to outliers)\n",
    "df_cleaned = df.copy()\n",
    "df_cleaned['Age'] = df_cleaned['Age'].fillna(df_cleaned['Age'].median())\n",
    "df_cleaned['Salary'] = df_cleaned['Salary'].fillna(df_cleaned['Salary'].median())\n",
    "\n",
    "# For categorical: use mode\n",
    "df_cleaned['Department'] = df_cleaned['Department'].fillna(df_cleaned['Department'].mode()[0])\n",
    "\n",
    "print(\"Data after Missing Value Imputation:\")\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with different scales\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3) * np.array([1000, 10, 100])\n",
    "df_scaled = pd.DataFrame(X, columns=['Income', 'Age', 'Score'])\n",
    "\n",
    "print(\"Original Data (different scales):\")\n",
    "print(df_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why StandardScaler?\n",
    "\n",
    "StandardScaler (z-score normalization) is used when algorithm assumes **zero-centered data** or uses **distance-based metrics** (KNN, SVM, Neural Networks). It transforms data to mean=0, std=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler - transforms to mean=0, std=1\n",
    "scaler_standard = StandardScaler()\n",
    "X_standard = scaler_standard.fit_transform(df_scaled)\n",
    "\n",
    "print(\"After StandardScaler:\")\n",
    "print(pd.DataFrame(X_standard, columns=df_scaled.columns).describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why MinMaxScaler?\n",
    "\n",
    "MinMaxScaler is used when we need **bounded values** (0-1 range) or for **algorithms that don't assume normality** like Neural Networks and K-Nearest Neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler - transforms to range [0, 1]\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(df_scaled)\n",
    "\n",
    "print(\"After MinMaxScaler:\")\n",
    "print(pd.DataFrame(X_minmax, columns=df_scaled.columns).describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample categorical data\n",
    "df_cat = pd.DataFrame({\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue'],\n",
    "    'Size': ['S', 'M', 'L', 'S', 'M'],\n",
    "    'Label': [1, 0, 1, 0, 1]\n",
    "})\n",
    "\n",
    "print(\"Categorical Data:\")\n",
    "print(df_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Label Encoding?\n",
    "Label Encoding converts categories to numbers (0, 1, 2...). It's suitable for **ordinal variables** or when the algorithm can learn **ordinal relationships** (Decision Trees, Random Forest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding - for ordinal/nominal with tree-based algorithms\n",
    "le = LabelEncoder()\n",
    "df_cat['Color_encoded'] = le.fit_transform(df_cat['Color'])\n",
    "\n",
    "print(\"After Label Encoding:\")\n",
    "print(df_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why One-Hot Encoding?\n",
    "One-Hot Encoding creates binary columns for each category. It's suitable for **nominal variables** without ordinal relationship and for **linear models** where numbers shouldn't imply order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding - for nominal variables\n",
    "df_onehot = pd.get_dummies(df_cat[['Color', 'Size']], drop_first=False)\n",
    "\n",
    "print(\"After One-Hot Encoding:\")\n",
    "print(df_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Pipeline & ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "X = pd.DataFrame({\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 22, 28],\n",
    "    'Income': [50000, 60000, 55000, 70000, 80000, 90000, 45000, 52000],\n",
    "    'City': ['NYC', 'LA', 'NYC', 'Chicago', 'LA', 'Chicago', 'NYC', 'LA']\n",
    "})\n",
    "y = [1, 0, 1, 0, 0, 1, 1, 0]\n",
    "\n",
    "print(\"Sample Dataset:\")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Pipeline?\n",
    "Pipeline chains multiple transformations and ensures **consistent preprocessing** for both training and test data, prevents **data leakage** by fitting only on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing for numeric and categorical columns\n",
    "numeric_features = ['Age', 'Income']\n",
    "categorical_features = ['City']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply transformation\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "print(\"Transformed Data Shape:\", X_transformed.shape)\n",
    "print(\"\\nTransformed Data (first 5 rows):\")\n",
    "print(X_transformed[:5].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Technique | When to Use | Why |\n",
    "|-----------|-------------|-----|\n",
    "| Mean/Median Imputation | MCAR data, numerical features | Mean for normal data, Median for outliers |\n",
    "| StandardScaler | Distance-based algorithms | Zero-centered, unit variance |\n",
    "| MinMaxScaler | Bounded data needed, Neural Networks | Maps to [0,1] range |\n",
    "| Label Encoding | Ordinal variables, Tree algorithms | Preserves order |\n",
    "| One-Hot Encoding | Nominal variables, Linear models | No false ordinal relationship |\n",
    "| Pipeline | Any ML workflow | Prevents leakage, ensures consistency |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
