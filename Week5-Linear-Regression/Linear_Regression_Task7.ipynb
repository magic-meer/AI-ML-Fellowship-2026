{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Linear Regression\n",
    "\n",
    "## Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning?\n",
    "\n",
    "Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn from data and make predictions without being explicitly programmed. Instead of following explicit instructions, ML algorithms identify patterns in data and use them to make decisions.\n",
    "\n",
    "### Types of Machine Learning\n",
    "\n",
    "1. **Supervised Learning**: Learning from labeled data (input-output pairs)\n",
    "   - Classification: Predicting discrete categories\n",
    "   - Regression: Predicting continuous values\n",
    "\n",
    "2. **Unsupervised Learning**: Finding patterns in unlabeled data\n",
    "   - Clustering, Dimensionality Reduction\n",
    "\n",
    "3. **Reinforcement Learning**: Learning through rewards and penalties\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear Regression is a **supervised learning** algorithm used for **regression** tasks. It models the relationship between a dependent variable (Y) and one or more independent variables (X) by fitting a linear equation.\n",
    "\n",
    "**Simple Linear Regression**: $y = mx + b$\n",
    "\n",
    "**Multiple Linear Regression**: $y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$\n",
    "\n",
    "Where:\n",
    "- $y$ is the target variable\n",
    "- $x$ is the feature(s)\n",
    "- $m$ or $b_1, b_2, ...$ are coefficients (weights)\n",
    "- $b$ or $b_0$ is the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for linear regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# Features: Years of Experience\n",
    "X = np.random.rand(100, 1) * 10  # 100 samples, values between 0-10\n",
    "\n",
    "# Target: Salary (with some noise)\n",
    "# True relationship: Salary = 30000 + 5000 * Experience + noise\n",
    "y = 30000 + 5000 * X + np.random.randn(100, 1) * 5000\n",
    "\n",
    "# Flatten for easier handling\n",
    "X = X.flatten()\n",
    "y = y.flatten()\n",
    "\n",
    "print(\"Feature (Years of Experience) shape:\", X.shape)\n",
    "print(\"Target (Salary) shape:\", y.shape)\n",
    "print(\"\\nFirst 10 samples:\")\n",
    "for i in range(10):\n",
    "    print(f\"Experience: {X[i]:.2f}, Salary: ${y[i]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.title('Years of Experience vs Salary')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implement Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Using Normal Equation (Closed-form Solution)\n",
    "\n",
    "The normal equation provides a closed-form solution for linear regression:\n",
    "\n",
    "$\\theta = (X^T X)^{-1} X^T y$\n",
    "\n",
    "This method directly computes the optimal weights without iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionFromScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation using the Normal Equation\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using the Normal Equation\n",
    "        \n",
    "        Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        y: numpy array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Add bias term (column of ones)\n",
    "        X_b = np.c_[np.ones((n_samples, 1)), X]\n",
    "        \n",
    "        # Normal Equation: theta = (X^T * X)^-1 * X^T * y\n",
    "        # Using np.linalg.pinv for numerical stability\n",
    "        theta = np.linalg.pinv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "        self.bias = theta[0]\n",
    "        self.weights = theta[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \n",
    "        Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        predictions: numpy array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        return self.bias + X @ self.weights\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return model parameters\"\"\"\n",
    "        return {\n",
    "            'intercept': self.bias,\n",
    "            'coefficient': self.weights\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Using Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGradientDescent:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation using Gradient Descent\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def _compute_cost(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error cost function\n",
    "        \n",
    "        J = (1/2n) * Σ(y_pred - y)^2\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        y_pred = self.predict(X)\n",
    "        cost = (1 / (2 * n)) * np.sum((y_pred - y) ** 2)\n",
    "        return cost\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using Gradient Descent\n",
    "        \n",
    "        Parameters:\n",
    "        X: numpy array of shape (n_samples, n_features)\n",
    "        y: numpy array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize weights and bias to zero\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient Descent optimization\n",
    "        for i in range(self.n_iterations):\n",
    "            # Make prediction\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Record cost for visualization\n",
    "            if i % 100 == 0:\n",
    "                cost = self._compute_cost(X, y)\n",
    "                self.cost_history.append(cost)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return model parameters\"\"\"\n",
    "        return {\n",
    "            'intercept': self.bias,\n",
    "            'coefficients': self.weights\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Reshape for our implementation\n",
    "X_train_reshaped = X_train.reshape(-1, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train Linear Regression from Scratch (Normal Equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using Normal Equation implementation\n",
    "model_normal = LinearRegressionFromScratch()\n",
    "model_normal.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_normal = model_normal.predict(X_test)\n",
    "\n",
    "# Get parameters\n",
    "params_normal = model_normal.get_params()\n",
    "print(\"=== Linear Regression (Normal Equation) ===\")\n",
    "print(f\"Intercept (bias): {params_normal['intercept']:,.2f}\")\n",
    "print(f\"Coefficient: {params_normal['coefficient'][0]:,.2f}\")\n",
    "print(f\"\\nEquation: y = {params_normal['intercept']:,.2f} + {params_normal['coefficient'][0]:,.2f} * x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train Linear Regression from Scratch (Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using Gradient Descent implementation\n",
    "model_gd = LinearRegressionGradientDescent(\n",
    "    learning_rate=0.01, \n",
    "    n_iterations=1000\n",
    ")\n",
    "model_gd.fit(X_train_reshaped, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gd = model_gd.predict(X_test_reshaped)\n",
    "\n",
    "# Get parameters\n",
    "params_gd = model_gd.get_params()\n",
    "print(\"=== Linear Regression (Gradient Descent) ===\")\n",
    "print(f\"Intercept (bias): {params_gd['intercept']:,.2f}\")\n",
    "print(f\"Coefficient: {params_gd['coefficients'][0]:,.2f}\")\n",
    "print(f\"\\nEquation: y = {params_gd['intercept']:,.2f} + {params_gd['coefficients'][0]:,.2f} * x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using Scikit-Learn\n",
    "model_sklearn = LinearRegression()\n",
    "model_sklearn.fit(X_train_reshaped, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_sklearn = model_sklearn.predict(X_test_reshaped)\n",
    "\n",
    "print(\"=== Linear Regression (Scikit-Learn) ===\")\n",
    "print(f\"Intercept (bias): {model_sklearn.intercept_:,.2f}\")\n",
    "print(f\"Coefficient: {model_sklearn.coef_[0]:,.2f}\")\n",
    "print(f\"\\nEquation: y = {model_sklearn.intercept_:,.2f} + {model_sklearn.coef_[0]:,.2f} * x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate and print regression metrics\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n=== {model_name} ===\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:,.2f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:,.2f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:,.2f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    return {'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results_normal = evaluate_model(y_test, y_pred_normal, \"Normal Equation (From Scratch)\")\n",
    "results_gd = evaluate_model(y_test, y_pred_gd, \"Gradient Descent (From Scratch)\")\n",
    "results_sklearn = evaluate_model(y_test, y_pred_sklearn, \"Scikit-Learn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Normal Equation', 'Gradient Descent', 'Scikit-Learn'],\n",
    "    'Intercept': [params_normal['intercept'], params_gd['intercept'], model_sklearn.intercept_],\n",
    "    'Coefficient': [params_normal['coefficient'][0], params_gd['coefficients'][0], model_sklearn.coef_[0]],\n",
    "    'MSE': [results_normal['MSE'], results_gd['MSE'], results_sklearn['MSE']],\n",
    "    'RMSE': [results_normal['RMSE'], results_gd['RMSE'], results_sklearn['RMSE']],\n",
    "    'MAE': [results_normal['MAE'], results_gd['MAE'], results_sklearn['MAE']],\n",
    "    'R2': [results_normal['R2'], results_gd['R2'], results_sklearn['R2']]\n",
    "})\n",
    "\n",
    "print(\"=== Model Comparison ===\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: All models comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='Actual', edgecolors='black')\n",
    "plt.plot(X_test, y_pred_normal, color='red', linewidth=2, label='Normal Equation')\n",
    "plt.plot(X_test, y_pred_gd, color='green', linewidth=2, linestyle='--', label='Gradient Descent')\n",
    "plt.plot(X_test, y_pred_sklearn, color='orange', linewidth=2, linestyle=':', label='Scikit-Learn')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.title('Predictions Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Gradient Descent Cost History\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(0, 1000, 100), model_gd.cost_history, color='purple', linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Gradient Descent Cost History')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Residuals\n",
    "plt.subplot(1, 3, 3)\n",
    "residuals = y_test - y_pred_sklearn\n",
    "plt.scatter(y_pred_sklearn, residuals, color='brown', alpha=0.6)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot (Scikit-Learn)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multiple Linear Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's demonstrate multiple linear regression with more than one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for multiple linear regression\n",
    "# Predicting house prices based on: size, bedrooms, age\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "size = np.random.rand(n_samples) * 2000 + 500  # Size in sq ft\n",
    "bedrooms = np.random.randint(1, 6, n_samples)  # Number of bedrooms\n",
    "age = np.random.randint(0, 50, n_samples)  # Age of house\n",
    "\n",
    "# Target: House price\n",
    "# True relationship: Price = 50000 + 150*size + 10000*bedrooms - 500*age + noise\n",
    "price = 50000 + 150 * size + 10000 * bedrooms - 500 * age + np.random.randn(n_samples) * 20000\n",
    "\n",
    "# Create feature matrix\n",
    "X_multi = np.column_stack((size, bedrooms, age))\n",
    "y_multi = price\n",
    "\n",
    "print(\"Multiple Linear Regression Data:\")\n",
    "print(f\"Features: Size (sq ft), Bedrooms, Age (years)\")\n",
    "print(f\"Target: House Price\")\n",
    "print(f\"\\nSample data (first 5 rows):\")\n",
    "for i in range(5):\n",
    "    print(f\"Size: {X_multi[i, 0]:.0f}, Bedrooms: {X_multi[i, 1]}, Age: {X_multi[i, 2]:.0f} -> Price: ${y_multi[i]:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X_multi, y_multi, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train using our implementation\n",
    "model_multi = LinearRegressionFromScratch()\n",
    "model_multi.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Train using Scikit-Learn\n",
    "model_multi_sklearn = LinearRegression()\n",
    "model_multi_sklearn.fit(X_train_multi, y_train_multi)\n",
    "\n",
    "# Predictions\n",
    "y_pred_multi = model_multi.predict(X_test_multi)\n",
    "y_pred_multi_sklearn = model_multi_sklearn.predict(X_test_multi)\n",
    "\n",
    "print(\"=== Multiple Linear Regression Results ===\")\n",
    "print(\"\\nFrom Scratch (Normal Equation):\")\n",
    "print(f\"Intercept: ${model_multi.bias:,.2f}\")\n",
    "print(f\"Coefficients: Size=${model_multi.weights[0]:,.2f}, Bedrooms=${model_multi.weights[1]:,.2f}, Age=${model_multi.weights[2]:,.2f}\")\n",
    "\n",
    "print(\"\\nScikit-Learn:\")\n",
    "print(f\"Intercept: ${model_multi_sklearn.intercept_:,.2f}\")\n",
    "print(f\"Coefficients: Size=${model_multi_sklearn.coef_[0]:,.2f}, Bedrooms=${model_multi_sklearn.coef_[1]:,.2f}, Age=${model_multi_sklearn.coef_[2]:,.2f}\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\n=== Model Evaluation ===\")\n",
    "print(f\"\\nFrom Scratch - R² Score: {r2_score(y_test_multi, y_pred_multi):.4f}\")\n",
    "print(f\"Scikit-Learn - R² Score: {r2_score(y_test_multi, y_pred_multi_sklearn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Learned:\n",
    "\n",
    "1. **Linear Regression** is a fundamental supervised learning algorithm for regression tasks\n",
    "\n",
    "2. **Normal Equation** provides a closed-form solution:\n",
    "   - Advantages: Exact solution, no iteration needed\n",
    "   - Disadvantages: Computationally expensive for large datasets, matrix inversion can be unstable\n",
    "\n",
    "3. **Gradient Descent** provides an iterative solution:\n",
    "   - Advantages: Works well with large datasets, handles complexity better\n",
    "   - Disadvantages: Requires hyperparameter tuning (learning rate, iterations)\n",
    "\n",
    "4. **Scikit-Learn** provides optimized, production-ready implementation\n",
    "   - Use for real-world applications\n",
    "   - Understanding the math helps in debugging and optimization\n",
    "\n",
    "5. **Evaluation Metrics**:\n",
    "   - MSE/RMSE: Penalizes large errors more\n",
    "   - MAE: More robust to outliers\n",
    "   - R²: Explains variance in the data (0-1, higher is better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- ✅ Introduction to Machine Learning concepts\n",
    "- ✅ Linear Regression implementation from scratch (Normal Equation)\n",
    "- ✅ Linear Regression implementation from scratch (Gradient Descent)\n",
    "- ✅ Linear Regression using Scikit-Learn\n",
    "- ✅ Model evaluation and comparison\n",
    "- ✅ Multiple Linear Regression example\n",
    "- ✅ Data visualization and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
